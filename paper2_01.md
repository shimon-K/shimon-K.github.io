# Introduction

Not only that "Learning" as defined in ML is simply an identification
process based on sophistical but limited memorizing, without involving
any thinking or imagination. But reinforcement learning is also type of
a low/narrow intelligence, which exists also in animals, such as in
Pablo’s dog experiment, where learning were taught by positive (reward)
and negative (punishment) feedbacks.

ML is still a part of AI for a good reason - it replaces the need for
the user to know and be familiar with the problem extensively , and
gives a general method that can find good solutions from example
training. Also, this is kind of a higher type of programming - where no
explicit coding is done.

However one must remember that studies showed that for a successful ML
we need to know the system, also ML is a very task-specific strategy.
Also ML is a static structure/model imitation of the brain, while there
is the effect of dynamical processes, such as construction/destruction
of neurons. Moreover, it is not fair to compare performances of ML to a
regular control-based method designed by a human  or to the use of
statistical methods , after studying the system. The reason is that ML
know the problem only from a limited set of examples, that frequently do
not represent all situations, but it also lack the comprehensive vision
as a human has. Hence, this is another reason for the necessity with
conclusion-generating AI.

Also the curse of dimensionality (COD) says that as the input vector of
information is larger, then we require exponentially more examples to
keep the same performance level. <span style="color: red">That’s
why</span>But downsizing dimensions methods because of a small number of
available examples result also in poorer performance.

But this is because of the wrong attitude towards AI. If we’d relax the
performance or replace it totally with other criteria to judge AI, then
we’ll exchange the desire to fit some NN model to the data (by enormous
amount of examples) and thus fixing this model to represent this
specific data and nothing else, with enabling freedom for the AI to
generalize and process (by less amount of examples).

<span class="roman">  
</span><span class="roman">  
</span> ... Many DNN’s are developed for a very specific tasks in
relation to visual information. Such features for example as a change in
light, different angles, recognition of boundaries of objects in an
image. And all that only on static photos, before we consider to add
movement and video type of change in time. The amount of work and theory
developed around computer vision area is huge, but I don’t consider this
as an effective method to use real intelligence. Since we doing it all
again as we did with simple model algorithms: programming very big code
to handle for example a task of computer recognizing space or a robotic
arm movement task, to take into account all possible scenarios.

Or the fact that most of the thinking and planning occurs in the human
part taking, and in the machine end all is left is an execution.

But this is not a progress. We should learn the basic real intelligent
learning as it occurs in human, without these complex programming, the
way we distinguish and recognize objects in a changing environment. And
we have to build an AI such that it isn’t expected to be stable or
perfect right away, immediately at first execution. This is a wrong
attitude towards actual intelligence.

On the contrary, we have to give it the time to develop, just as an
infant baby and a child do, and not demand from it to give always
"correct" answers, but instead - as a human does - allow it to make a
mistakes based on partial understanding and learn from it not as a new
input in DNN, but as another step in a more general and mature step of
development. As a more general point of view on the data he encountered
during its lifetime.

In other words, as the comparison between serial thinking verse parallel
one  show, that in serial one we prefer judgment and quick selection or
fast results with emphasis on certainty, in parallel however it is
fluent and non-judgmental in favor of fluency and multiple solutions and
options together with their probabilities, i.e. allowing and welcoming
uncertainty instead of fighting it. The same is here, model-based
methods and control are mostly designed for fast good results, to prove
effectiveness. But human intelligence shows, that it takes years for an
infant to gather linguistic capabilities and fine motor sensing. It
takes a many months, in which the baby is mumbles or pronounce poorly
and have a gross motor skills (i.e. in movement and drawing). This
observation support the idea that the more AGI is general, to deal with
as variant knowledge as possible, the more efforts it take to adapt and
learn this knowledge. And the opposite is true also - the more the AGI
is specific, like current control methods, than the more it fits to be
effective in special data and faster in results.

This is actually the difference between efficiency and effectiveness ,
where efficiency concentrates on the best exploitation of available
resources, while effectiveness is about performance measure of how well
the goal is achieved. Consequently, the AI must be efficient more than
effective, since we’re less interested in some specific desired
outcomes, but rather a good thinking machine which can be validated only
on the long run. Same idea expressed within the comparison between
serial and parallel thinking , where using parallel thinking means a
more general, comprehensive and planning, including the freedom and the
space to explore, taking intelligent risks and long trial-and-error. In
contrast, we have demand for perfection and immediate success in serial
thinking. In other words, a broader view to see how one can be more
effective in the bigger picture rather than in some specific and narrow
tasks.

We shouldn’t forget that we mimic the next best thing. Just like we
imitated many processes in nature such as GA, flying, and many more -
what a man engineered frequently had a poorer quality. Because usually
these versions were only a simplified copies of the real thing. So how
can we expect so much more from an imitation of human brain activity?
With so little knowledge about it yet, and yet expect it to perform
better than the real thing? Better than human.
