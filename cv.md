---
layout: page
title: Curriculum vitae (CV)
---


## My evolution

- Started with learning physics.
- Continued with educational physics, with the purpose to build a computer software to serve as a tool to provide a more comprehensible and more understandable physics. It is called SciSoft.
<br>Partcicpated in Prof. Miriam Reiner's <a style="color:#8dd3c7" href="https://vrneurocog.wixsite.com/vrneurocog/researchers">NeuroCognition in Education Lab</a>.<br>Though the tool was not expressed practically, its theoretical framework can be used later in AGI.
- Then continued with totally different field: Optimal control applied in transportation.<br>Partcipated in Prof. Jack Haddad's <a style="color:#8dd3c7" href="https://haddad.net.technion.ac.il/lab-members/">TSMART Lab</a>.
- Then moved on into AI field, applying Deep Learning (Supervised Learning via Graph Neural Network and Reinforcement Learning via Deep Q-Learning) in signal control problem at transportation network.
- During my studying the AI field, then Machine Learning and then Deep Learning, I encountered the fabulous and fascinating world of Artificial General Intelligence (AGI).


## Overview of gained knowledge


**Note**: All formal courses were taken in [Technion](https://www.jpost.com/business-and-innovation/all-news/article-717204) [Institute](https://www.calcalist.co.il/calcalistech/article/bkkte6gbo?fbclid=IwAR20MX1Z7Bkiz5yueRLk2s0RiWB5944RNntQTAKW0lJroIRzTEyldoFe6Ro) in Israel.


### Control Theory:

<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th style="width: 2%">Courses</th>
      <th style="width: 4%" align="left">Title</th>
      <th align="left">Main Ideas</th>
      <!--<th align="left">Resources</th>
      <th align="left">Internal ref</th>-->
    </tr>
  </thead>
  <tbody>
<!-- =============================== Formal Courses ================================ -->
    <tr>
      <td rowspan="2" align="center" class="vertical-th" style="font-size:30px">Formal</td>
      <td rowspan="1">Optimal control</td>
      <td rowspan="1" style="font-size:12px">Pontryagin Maximum Principle (PMP), Lagrange multipliers, Sliding mode, Linear control,<br />Linear Quadratic Regulator (LQR), Continous Knapsack Problem (CKP), Krotov Lemma</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
    <tr>
      <td rowspan="1">Non-Linear control</td>
      <td rowspan="1" style="font-size:12px">1st-, 2nd- and High-Order Systems, Phase Diagram, State Space, Region Of Attraction, Systems: (Non-)Autonoums, Perturbed, Open/Closed-Loop, with input/output - and their Lyapunov stability conditions. BIBO, ISS, Canonical Forms: Normal and Controller - and their Control Design. Causality, Dynamic Systems, Tracking, Sliding Mode Control (SMC)</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
<!-- =============================== Self-learning Courses ================================ -->
    <tr>
      <td rowspan="6" align="center" class="vertical-th" style="font-size:30px">Self learning</td>
      <td rowspan="1">System Analysis</td>
      <td rowspan="1" style="font-size:12px">Convexity, Linear Programming (LP), Simplex, KKT conditions, Primal/Dual Problem,<br />(Mixed) Integer LP, Dynamic programming (DP), Decision under uncertainty</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
    <tr>
      <td rowspan="1">Linear Systems</td>
      <td rowspan="1" style="font-size:12px">Discrete/Continuous Systems, Transformation: Laplace, Z, Fourier. Time/Frequency domain. Canonical Forms: Observer, Controller. Stationarity, Convolution, Different-order Systems, Feedback Control, LTI, SISO, Stability analysis: poles and zeros, Transient and Steady-state. Bode Plot, Routh–Hurwitz stability criterion, Asymptotic stabilities, Impulse response</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
    <tr>
      <td rowspan="1">Introduction to Control</td>
      <td rowspan="1" style="font-size:12px">Block diagram, Open/Closed-loop control, Plant Inversion, Internal Stability,<br />Root-Locus method, Nyquist stability criterion, Delay/Dead-time, Gain, Lead/lag Compensator,<br />Basic loop shaping and stability margins, PID controllers</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
    <tr>
      <td rowspan="1">Control Theory</td>
      <td rowspan="1" style="font-size:12px">MIMO, Advanced loop shaping, Pole placement, Nychols Chart, Kalman Filtering, LQG,<br />Sampled-data control: connecting analog and digital, control of dicrete-time systems, aliasing. Controllability, Observability, Estimators, Robustness, Transfer functions</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
    <tr>
      <td rowspan="1">Adaptive Control</td>
      <td rowspan="1" style="font-size:12px">Signal/System Norm, Lyapunov stability: <!--La Salle’s Lemma or -->Invariant sets theorem, Time Varying System Stability, Barablat’s Lemma. Direct control: model reference adaptive control (MRAC), Indirect control: self-tuning-regulator (STR). Linearization, Sliding Variables, Robust Control, Dynamic Inversion</td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>    
    <tr>
      <td rowspan="1">Systems and Control</td>
      <td rowspan="1" style="font-size:12px">Quadratic Programming (QP), Multiparametric Programming, Model Predictive Control (MPC), Receding Horizon Control (RHC), Vertex Control, Set theory: Ellipsoidal/Polyhedral set, Nominal/Robust State/Output feedback Interpolating control (IC), Interpolating with elliptic-sets/cost, Constrained optimal control, Linear Matrix Inequality (LMI), Implicit/Explicit solution<!--, Tracking control--></td>
      <!--<td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>-->
    </tr>
  </tbody>
</table>


### Other courses completed in Technion: 
1. Statistics (Linear Regression, Random variables, Hypothesis Testing, Sampling, Estimation, Monte-Carlo models, ANOVA)
2. Traffic control
3. Transportation Engineering and Management
4. Demand Modelling (Discrete Choice Methods, Maximum Likelihod Estimation (MLE), Hypothesis Testing, Logit, Probit, Utility)
5. Transportation Network Analysis (Equilibrium Problem, Optimal routing, Route Choice Models)
6. Transportation Planning
7. Network theory (Maximum flow problem, Shortest-Path problem, Minimum spanning tree)
8. Advanced Transportation Engineering (Microscopic/mesoscopic/macroscopic traffic models, Macroscopic Fundamental Diagram).


### Artificial Intelligence:


<table>
<!-- =============================== HEADER ================================ -->
  <thead>
    <tr>
      <th style="width:10px">Type</th>
      <th style="width:40px" align="left">Title</th>
      <th style="width:350px" align="left">Main Ideas</th>
      <th style="width:20px" align="left">Resources</th>
      <th style="width:10px">Internal ref</th>
    </tr>
  </thead>
  <tbody>
<!-- =============================== Formal Courses ================================ -->
    <tr>
      <td rowspan="3" align="center" class="vertical-th" style="font-size:30px">Formal Courses</td>
      <td rowspan="1">Introduction to Machine Learning</td>
      <td rowspan="1" style="font-size:12px">--</td>
      <td rowspan="1">-res-</td>
      <td rowspan="1">-ref-</td>
    </tr>
    <tr>
      <td rowspan="1">Deep Learning</td>
      <td rowspan="1" style="font-size:12px">PAC Bayes, dropout, KL/JS divergence, Batch Normalization, Adaptive learning, CNNs (ResNet, GoogleNet, ..), RNNs (LSTM, GRU), Transformers (attention, GNN, multi-modal, VQA), Generative learning ((Style/Cycle) GAN, discrete/continuous VAE, Gradient Langevin dynamics), Reinforcement Learning (value-based, policy-gradient, Actor-Critic (AC), A2C, A3C), Explainability (LIME, SHAP, Gradient-based)</td>
      <td rowspan="1">-res-</td>
      <td rowspan="1" class="vertical-th" align="center">DL.docx</td>
    </tr>
    <tr>
      <td rowspan="1">Introduction to Natural Language Processing</td>
      <td rowspan="1" style="font-size:12px">Set/Bag of words, Distance metrics (Hamming/Jaccard/Euclidean/Cosine), Classification (Majority, Neirest Neighbor, Naïve-Bayes), Text normalization (Tokenization, Lemmatization, stemming), Regular expression (RE), true/false positive/negative, POR plot, Cross-validation, bootstrap, Laplace smoothing, N-gram/Neural Language model (LM), perplexity, Hidden Markov model (HMM), Word Embeddings (word2vec), POS tagging, named entity recognition (NER), Viterbi algorithm, (Probabilistic/Weighted) context-free grammar (CFG), Chomsky normal form, Semantic Parsing, lambda calculus, Seq2Seq (encoder-decoder, attention), Quantifiers, Contextual Embeddings (BERT, BART), beam search, machine translation (MT)</td>
      <td rowspan="1">-res-</td>
      <td rowspan="1" class="vertical-th" align="center">Intro_to_NLP.docx</td>
    </tr>    
<!-- =============================== Self-learning Courses ================================ -->
    <tr>
      <td rowspan="6" align="center" class="vertical-th" style="font-size:30px">Self learning Courses</td>
      <td rowspan="1">Deep Learning in Computer Vision</td>
      <td rowspan="1" style="font-size:12px">different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO)</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/deep-learning-in-computer-vision/home/week/1">Coursera</a></td>
      <td class="vertical-th" rowspan="2" align="center">CNN_images.docx</td>
    </tr>
    <tr>
      <td rowspan="1">The Ancient Secrets of Computer Vision</td>
      <td rowspan="1" style="font-size:12px">different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO)</td>
      <td rowspan="1"><a href="https://www.youtube.com/watch?v=8jXIAWg_yHU&list=PLjMXczUzEYcHvw5YYSU92WrY8IwhTuq7p">Course</a></td>
    </tr>
    <tr>
      <td rowspan="1">Machine Learning courses</td>
      <td rowspan="1" style="font-size:12px">Block diagram, Open/Closed-loop control, Plant Inversion, Internal Stability,<br />Root-Locus method, Nyquist stability criterion, Delay/Dead-time, Gain, Lead/lag Compensator,<br />Basic loop shaping and stability margins, PID controllers</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/machine-learning-duke">1</a>,<a href="https://www.coursera.org/learn/linear-algebra-machine-learning/home/welcome">2</a>,<a href="https://www.youtube.com/watch?v=MEG35RDD7RA">3</a></td>
      <td class="vertical-th" rowspan="3" align="center">AICourses.docx</td>
    </tr>
    <tr>
      <td rowspan="1">Deep Learning courses</td>
      <!-- [1](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome), [2](https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&index=1) -->
      <td rowspan="1" style="font-size:12px">MIMO, Advanced loop shaping, Pole placement, Nychols Chart, Kalman Filtering, LQG,<br />Sampled-data control: connecting analog and digital, control of dicrete-time systems, aliasing. Controllability, Observability, Estimators, Robustness, Transfer functions</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome">1</a>,<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&index=1">2</a></td>
    </tr>
    <tr>
      <td rowspan="1">Connectionism in GOFAI or classical AI (20th century)</td>
      <!--  in [Youtube](https://www.youtube.com/watch?v=xbYgKoG4x2g&list=PL53BE265CE4A6C056) -->
      <td rowspan="1" style="font-size:12px">Basic learning rules, e.g. Hebbian; Associative memory; RBF (radial basis function); MLP (multi-layered perceptron); SOM (self-organizing-map)</td>
      <td rowspan="1"><a href="https://www.youtube.com/watch?v=xbYgKoG4x2g&list=PL53BE265CE4A6C056">Youtube</a></td>
    </tr>    
    <tr>
      <td rowspan="1">Systems and Control</td>
      <td rowspan="1" style="font-size:12px">Quadratic Programming (QP), Multiparametric Programming, Model Predictive Control (MPC), Receding Horizon Control (RHC), Vertex Control, Set theory: Ellipsoidal/Polyhedral set, Nominal/Robust State/Output feedback Interpolating control (IC), Interpolating with elliptic-sets/cost, Constrained optimal control, Linear Matrix Inequality (LMI), Implicit/Explicit solution<!--, Tracking control--></td>
      <!--<td rowspan="1">-res-</td>-->
      <td rowspan="1">-ref-</td>
    </tr>

<!-- =============================== Books and Articles ================================ -->
    <tr>
      <td rowspan="6" align="center" class="vertical-th" style="font-size:30px">Books and Articles</td>
      <td rowspan="1">Deep Learning in Computer Vision</td>
      <td rowspan="1" style="font-size:12px">different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO)</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/deep-learning-in-computer-vision/home/week/1">Coursera</a></td>
      <td class="vertical-th" rowspan="2" align="center">CNN_images.docx</td>
    </tr>
    <tr>
      <td rowspan="1">The Ancient Secrets of Computer Vision</td>
      <td rowspan="1" style="font-size:12px">different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO)</td>
      <td rowspan="1"><a href="https://www.youtube.com/watch?v=8jXIAWg_yHU&list=PLjMXczUzEYcHvw5YYSU92WrY8IwhTuq7p">Course</a></td>
    </tr>
    <tr>
      <td rowspan="1">Machine Learning courses</td>
      <td rowspan="1" style="font-size:12px">Block diagram, Open/Closed-loop control, Plant Inversion, Internal Stability,<br />Root-Locus method, Nyquist stability criterion, Delay/Dead-time, Gain, Lead/lag Compensator,<br />Basic loop shaping and stability margins, PID controllers</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/machine-learning-duke">1</a>,<a href="https://www.coursera.org/learn/linear-algebra-machine-learning/home/welcome">2</a>,<a href="https://www.youtube.com/watch?v=MEG35RDD7RA">3</a></td>
      <td class="vertical-th" rowspan="3" align="center">AICourses.docx</td>
    </tr>
    <tr>
      <td rowspan="1">Deep Learning courses</td>
      <!-- [1](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome), [2](https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&index=1) -->
      <td rowspan="1" style="font-size:12px">MIMO, Advanced loop shaping, Pole placement, Nychols Chart, Kalman Filtering, LQG,<br />Sampled-data control: connecting analog and digital, control of dicrete-time systems, aliasing. Controllability, Observability, Estimators, Robustness, Transfer functions</td>
      <td rowspan="1"><a href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome">1</a>,<a href="https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&index=1">2</a></td>
    </tr>
    <tr>
      <td rowspan="1">Connectionism in GOFAI or classical AI (20th century)</td>
      <!--  in [Youtube](https://www.youtube.com/watch?v=xbYgKoG4x2g&list=PL53BE265CE4A6C056) -->
      <td rowspan="1" style="font-size:12px">Basic learning rules, e.g. Hebbian; Associative memory; RBF (radial basis function); MLP (multi-layered perceptron); SOM (self-organizing-map)</td>
      <td rowspan="1"><a href="https://www.youtube.com/watch?v=xbYgKoG4x2g&list=PL53BE265CE4A6C056">Youtube</a></td>
    </tr>    
    <tr>
      <td rowspan="1">Systems and Control</td>
      <td rowspan="1" style="font-size:12px">Quadratic Programming (QP), Multiparametric Programming, Model Predictive Control (MPC), Receding Horizon Control (RHC), Vertex Control, Set theory: Ellipsoidal/Polyhedral set, Nominal/Robust State/Output feedback Interpolating control (IC), Interpolating with elliptic-sets/cost, Constrained optimal control, Linear Matrix Inequality (LMI), Implicit/Explicit solution<!--, Tracking control--></td>
      <!--<td rowspan="1">-res-</td>-->
      <td rowspan="1">-ref-</td>
    </tr>
    
  </tbody>
</table>






<!--
AI self-learning courses:

--CNN_images.docx:                ALSO ADD RESOURCES, MAIN IDEAS LEARNED, AND WHERE STORED AS A TABLE--

- Deep Learning in Computer Vision – [Coursera](https://www.coursera.org/learn/deep-learning-in-computer-vision/home/week/1): different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO).
- The Ancient Secrets of Computer Vision ([Course](https://www.youtube.com/watch?v=8jXIAWg_yHU&list=PLjMXczUzEYcHvw5YYSU92WrY8IwhTuq7p)) - Joseph Redmon: different CNN architectures, Region-based CNNs (R-CNNs, SSD, YOLO).



--AICourses.docx:

- Machine Learning courses: [1](https://www.coursera.org/learn/machine-learning-duke), [2](https://www.coursera.org/learn/linear-algebra-machine-learning/home/welcome), [3](https://www.youtube.com/watch?v=MEG35RDD7RA), 
- Deep Learning courses: [1](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome), [2](https://www.youtube.com/watch?v=b99UVkWzYTQ&list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu&index=1), 
- Connectionism in GOFAI or classical AI (20th century) in [Youtube](https://www.youtube.com/watch?v=xbYgKoG4x2g&list=PL53BE265CE4A6C056): Basic learning rules, e.g. Hebbian; Associative memory; RBF (radial basis function); MLP (multi-layered perceptron); SOM (self-organizing-map);

-->

